{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d6656e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file 1...\n",
      "--2025-05-03 22:26:31--  https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/dataset_parser.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9788 (9.6K) [text/plain]\n",
      "Saving to: ‘dataset_parser.py.2’\n",
      "\n",
      "dataset_parser.py.2 100%[===================>]   9.56K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-05-03 22:26:31 (7.48 MB/s) - ‘dataset_parser.py.2’ saved [9788/9788]\n",
      "\n",
      "\n",
      "Downloading file 2...\n",
      "--2025-05-03 22:26:32--  https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/wiki_extractor.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6784 (6.6K) [text/plain]\n",
      "Saving to: ‘wiki_extractor.py.2’\n",
      "\n",
      "wiki_extractor.py.2 100%[===================>]   6.62K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-03 22:26:32 (14.9 MB/s) - ‘wiki_extractor.py.2’ saved [6784/6784]\n",
      "\n",
      "\n",
      "Downloading file 3...\n",
      "--2025-05-03 22:26:32--  https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/custom_dataset.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8062 (7.9K) [text/plain]\n",
      "Saving to: ‘custom_dataset.py.4’\n",
      "\n",
      "custom_dataset.py.4 100%[===================>]   7.87K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-03 22:26:32 (32.6 MB/s) - ‘custom_dataset.py.4’ saved [8062/8062]\n",
      "\n",
      "\n",
      "Downloading file 4...\n",
      "--2025-05-03 22:26:32--  https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/enwiki_features.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6694 (6.5K) [text/plain]\n",
      "Saving to: ‘enwiki_features.py’\n",
      "\n",
      "enwiki_features.py  100%[===================>]   6.54K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-03 22:26:33 (34.4 MB/s) - ‘enwiki_features.py’ saved [6694/6694]\n",
      "\n",
      "\n",
      "Download completati. Contenuto della directory corrente:\n"
     ]
    }
   ],
   "source": [
    "URL_FILE_1 = \"https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/dataset_parser.py\"\n",
    "URL_FILE_2 = \"https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/wiki_extractor.py\"\n",
    "URL_FILE_3 = \"https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/custom_dataset.py\"\n",
    "URL_FILE_4 = \"https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/enwiki_features.py\"\n",
    "\n",
    "print(\"Downloading file 1...\")\n",
    "!wget {URL_FILE_1}\n",
    "\n",
    "print(\"\\nDownloading file 2...\")\n",
    "!wget {URL_FILE_2}\n",
    "\n",
    "print(\"\\nDownloading file 3...\")\n",
    "!wget {URL_FILE_3}\n",
    "\n",
    "print(\"\\nDownloading file 4...\")\n",
    "!wget {URL_FILE_4}\n",
    "\n",
    "print(\"\\nDownload completati. Contenuto della directory corrente:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "909a4c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/leo/cultural_classifier/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: pandas in /home/leo/cultural_classifier/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/leo/cultural_classifier/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: wikidata in /home/leo/cultural_classifier/lib/python3.11/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /home/leo/cultural_classifier/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in /home/leo/cultural_classifier/lib/python3.11/site-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost pandas scikit-learn wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9654f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from custom_dataset import CustomData\n",
    "from enwiki_features import enwiki_augment # wikipedia page statistics (5 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e37dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the input file a validation set with a label column, used to check if the model is loading and working as intended?\n",
    "# Good for testing and final checks\n",
    "TEST_ON_VALID_SET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff03887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data non labeled...\n",
      "Adding feature...\n",
      "\n",
      "Fetch Summary:\n",
      "  Attempted: 300\n",
      "  Successful Fetches (Extractor created): 300\n",
      "  Failed Fetches (Extractor is None): 0\n",
      "Feature added...\n",
      "\n",
      "--- Preprocessing DataFrame (Initial rows: 300) ---\n",
      "Handling Missing Values: Nessuna riga con NaN trovata.\n",
      "--- Preprocessing Completo (Final rows: 300, Final columns: 32) ---\n",
      "Test data processed and saved in ~/content/test_unlabeled_processed.csv.\n",
      "\n",
      "--- Test processed ---\n",
      "Shape of preprocessed data: (300, 32)\n"
     ]
    }
   ],
   "source": [
    "# Percorsi file\n",
    "RAW_TEST_CSV        = '~/content/test_unlabeled.csv'\n",
    "PROCESSED_TEST_CSV  = '~/content/test_unlabeled_processed.csv'\n",
    "\n",
    "# Flag per forzare il ricalcolo\n",
    "FORCE_REPROCESS = False\n",
    "\n",
    "df_processed = None\n",
    "\n",
    "# --- Try to load the already processed file, if it exists ---\n",
    "if not FORCE_REPROCESS and os.path.exists(PROCESSED_TEST_CSV):\n",
    "    print(f\"Loading preprocessed data: {PROCESSED_TEST_CSV}...\")\n",
    "    try:\n",
    "        df_processed = pd.read_csv(PROCESSED_TEST_CSV)\n",
    "        print(\"Loading data done.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Loading error {PROCESSED_TEST_CSV}, starting recomputing: {e}\")\n",
    "        df_processed = None  # Force recomputing\n",
    "\n",
    "# --- If not existing or forced to, recompute the features ---\n",
    "if df_processed is None:\n",
    "    print(\"Processing test data non labeled...\")\n",
    "    try:\n",
    "        # 1. Load the initial CSV file\n",
    "        df_test_raw = pd.read_csv(RAW_TEST_CSV)\n",
    "        \n",
    "        # 2. Instantiate preprocessor and add features\n",
    "        processor_test = CustomData(df_test_raw)\n",
    "        df_test_featured = processor_test.add_feature()\n",
    "        \n",
    "        if df_test_featured is not None:\n",
    "            # 3. Apply preprocess (encoding, scaling, ecc.)\n",
    "            df_processed = processor_test.preprocess_data(df_test_featured)\n",
    "            \n",
    "            if df_processed is not None:\n",
    "                # 4. Save result on an output CSV\n",
    "                df_processed.to_csv(PROCESSED_TEST_CSV, index=False)\n",
    "                print(f\"Test data processed and saved in {PROCESSED_TEST_CSV}.\")\n",
    "            else:\n",
    "                print(\"WARNING: Test data is None or empty, saving failed.\")\n",
    "        else:\n",
    "            print(\"ERROR: Failed to add preprocessed features.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during data preprocessing: {e}\")\n",
    "        df_processed = None\n",
    "\n",
    "# --- final check ---\n",
    "if df_processed is not None:\n",
    "    print(\"\\n--- Test processed ---\")\n",
    "    print(f\"Shape of preprocessed data: {df_processed.shape}\")\n",
    "else:\n",
    "    print(\"\\nERROR: could not load preprocessed data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a719a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data shape: (300, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>page_length</th>\n",
       "      <th>num_links</th>\n",
       "      <th>mean_sitelinks_count</th>\n",
       "      <th>median_sitelinks_count</th>\n",
       "      <th>std_sitelinks_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2427430</td>\n",
       "      <td>Northeast Flag Replacement</td>\n",
       "      <td>concept</td>\n",
       "      <td>History</td>\n",
       "      <td>historical event</td>\n",
       "      <td>Northeast Flag Replacement</td>\n",
       "      <td>2156</td>\n",
       "      <td>18</td>\n",
       "      <td>68.055556</td>\n",
       "      <td>45.5</td>\n",
       "      <td>55.538247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q125482</td>\n",
       "      <td>imam</td>\n",
       "      <td>concept</td>\n",
       "      <td>philosophy and religion</td>\n",
       "      <td>religious leader</td>\n",
       "      <td>Imam</td>\n",
       "      <td>5134</td>\n",
       "      <td>62</td>\n",
       "      <td>71.516667</td>\n",
       "      <td>56.5</td>\n",
       "      <td>69.720033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15789</td>\n",
       "      <td>FC Bayern Munich</td>\n",
       "      <td>named entity</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports club</td>\n",
       "      <td>FC Bayern Munich</td>\n",
       "      <td>49066</td>\n",
       "      <td>369</td>\n",
       "      <td>48.817680</td>\n",
       "      <td>33.0</td>\n",
       "      <td>53.145687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q582496</td>\n",
       "      <td>Fome Zero</td>\n",
       "      <td>named entity</td>\n",
       "      <td>politics</td>\n",
       "      <td>government agency</td>\n",
       "      <td>Fome Zero</td>\n",
       "      <td>4791</td>\n",
       "      <td>25</td>\n",
       "      <td>56.583333</td>\n",
       "      <td>40.0</td>\n",
       "      <td>62.625818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q572811</td>\n",
       "      <td>Anthony Award</td>\n",
       "      <td>named entity</td>\n",
       "      <td>Literature</td>\n",
       "      <td>literary award</td>\n",
       "      <td>Anthony Awards</td>\n",
       "      <td>428</td>\n",
       "      <td>5</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.616844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      item                        name  \\\n",
       "0  http://www.wikidata.org/entity/Q2427430  Northeast Flag Replacement   \n",
       "1   http://www.wikidata.org/entity/Q125482                        imam   \n",
       "2    http://www.wikidata.org/entity/Q15789            FC Bayern Munich   \n",
       "3   http://www.wikidata.org/entity/Q582496                   Fome Zero   \n",
       "4   http://www.wikidata.org/entity/Q572811               Anthony Award   \n",
       "\n",
       "           type                 category        subcategory  \\\n",
       "0       concept                  History   historical event   \n",
       "1       concept  philosophy and religion   religious leader   \n",
       "2  named entity                   sports        sports club   \n",
       "3  named entity                 politics  government agency   \n",
       "4  named entity               Literature     literary award   \n",
       "\n",
       "                        title  page_length  num_links  mean_sitelinks_count  \\\n",
       "0  Northeast Flag Replacement         2156         18             68.055556   \n",
       "1                        Imam         5134         62             71.516667   \n",
       "2            FC Bayern Munich        49066        369             48.817680   \n",
       "3                   Fome Zero         4791         25             56.583333   \n",
       "4              Anthony Awards          428          5             15.500000   \n",
       "\n",
       "   median_sitelinks_count  std_sitelinks_count  \n",
       "0                    45.5            55.538247  \n",
       "1                    56.5            69.720033  \n",
       "2                    33.0            53.145687  \n",
       "3                    40.0            62.625818  \n",
       "4                    15.0             8.616844  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Percorsi file\n",
    "RAW_TEST_CSV        = '~/content/test_unlabeled.csv'\n",
    "PROCESSED_TEST_CSV  = '~/content/test_unlabeled_augmented.csv'\n",
    "\n",
    "# Flag to force data augmentation recomputing\n",
    "FORCE_REPROCESS = False\n",
    "\n",
    "df_aug = None\n",
    "\n",
    "# Load the augmented data if present\n",
    "if not FORCE_REPROCESS:\n",
    "    try:\n",
    "        df_aug = pd.read_csv(PROCESSED_TEST_CSV)\n",
    "        print(f\"Augmented data shape: {df_aug.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - could not load augmented data, trying to preprocess it: {e}\")\n",
    "\n",
    "# data augmentation from wikipedia scraping \n",
    "if FORCE_REPROCESS or df_aug is None:\n",
    "    enwiki_augment(RAW_TEST_CSV, PROCESSED_TEST_CSV, 10)\n",
    "    try:\n",
    "        df_aug = pd.read_csv(PROCESSED_TEST_CSV)\n",
    "        print(f\"Augmented data shape: {df_aug.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - could not load augmented data: {e}\")\n",
    "\n",
    "# Display the first few rows\n",
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4acfeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type',\n",
       " 'number_sitelinks',\n",
       " 'sitelinks_translation_entropy',\n",
       " 'number_claims',\n",
       " 'po_P495',\n",
       " 'po_P1343',\n",
       " 'po_P2596',\n",
       " 'po_P17',\n",
       " 'number_of_P31',\n",
       " 'sum_cultural_claims',\n",
       " 'po_P172',\n",
       " 'po_P1268',\n",
       " 'po_P136',\n",
       " 'category_Biology',\n",
       " 'category_Books',\n",
       " 'category_Comics and Anime',\n",
       " 'category_Fashion',\n",
       " 'category_Films',\n",
       " 'category_Geography',\n",
       " 'category_Gestures and habits',\n",
       " 'category_History',\n",
       " 'category_Literature',\n",
       " 'category_Music',\n",
       " 'category_Performing arts',\n",
       " 'category_Visual Arts',\n",
       " 'category_architecture',\n",
       " 'category_food',\n",
       " 'category_media',\n",
       " 'category_philosophy and religion',\n",
       " 'category_politics',\n",
       " 'category_sports',\n",
       " 'category_transportation']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented data with Custom Data\n",
    "df_processed.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5950778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item',\n",
       " 'name',\n",
       " 'type',\n",
       " 'category',\n",
       " 'subcategory',\n",
       " 'title',\n",
       " 'page_length',\n",
       " 'num_links',\n",
       " 'mean_sitelinks_count',\n",
       " 'median_sitelinks_count',\n",
       " 'std_sitelinks_count']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded augmented data from wikipedia scraping\n",
    "df_aug.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cddb3961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape train_aug_aligned dopo reindex: (300, 11)\n",
      "Features merged successfully\n",
      "type\n",
      "number_sitelinks\n",
      "sitelinks_translation_entropy\n",
      "number_claims\n",
      "po_p495\n",
      "po_p1343\n",
      "po_p2596\n",
      "po_p17\n",
      "number_of_p31\n",
      "sum_cultural_claims\n",
      "po_p172\n",
      "po_p1268\n",
      "po_p136\n",
      "category_biology\n",
      "category_books\n",
      "category_comics and anime\n",
      "category_fashion\n",
      "category_films\n",
      "category_geography\n",
      "category_gestures and habits\n",
      "category_history\n",
      "category_literature\n",
      "category_music\n",
      "category_performing arts\n",
      "category_visual arts\n",
      "category_architecture\n",
      "category_food\n",
      "category_media\n",
      "category_philosophy and religion\n",
      "category_politics\n",
      "category_sports\n",
      "category_transportation\n",
      "page_length\n",
      "num_links\n",
      "mean_sitelinks_count\n",
      "median_sitelinks_count\n",
      "std_sitelinks_count\n"
     ]
    }
   ],
   "source": [
    "# reindex the augmented data to match between the two files\n",
    "data_aug_aligned = df_aug.reindex(df_processed.index)\n",
    "print(f\"\\nShape train_aug_aligned dopo reindex: {data_aug_aligned.shape}\")\n",
    "\n",
    "# check if indexes have been matched correctly\n",
    "if df_processed.index.equals(df_aug.index):\n",
    "    print(\"Features merged successfully\")\n",
    "else:\n",
    "    print(\"ERROR: could not merge features by index\")\n",
    "\n",
    "# drop useless columns for classification scopes, merge the two sets of augmented data and features\n",
    "data_aug_aligned = data_aug_aligned.drop(columns=['item', 'name', 'type', 'category', 'subcategory', 'title'])\n",
    "df_val_aug_concat = pd.concat([df_processed, data_aug_aligned], axis=1)\n",
    "\n",
    "# optionally, if checking the model, drop the label column and prepare a y_val for evaluation\n",
    "if TEST_ON_VALID_SET:\n",
    "    data_aug_aligned = data_aug_aligned.drop(columns=['label'])\n",
    "    y_val = df_val_aug_concat['label']\n",
    "    df_val_aug_concat = df_val_aug_concat.drop(columns=['label'])\n",
    "\n",
    "# ensure that all the columns (the features names) are lower case\n",
    "df_val_aug_concat.columns = df_val_aug_concat.columns.str.lower()\n",
    "\n",
    "# display the final list of features \n",
    "features = df_val_aug_concat.columns.tolist()\n",
    "for name in features:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be2672ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape:\n",
      "  Data: (300, 37)\n"
     ]
    }
   ],
   "source": [
    "df = df_val_aug_concat.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"Final shape:\")\n",
    "print(f\"  Data: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7fd3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XGBoost model\n",
    "model_path = '~/content/xgb_best_model_77.json'\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)\n",
    "\n",
    "# Ensure that all the feature names are lower case and match the set columns\n",
    "booster.feature_names = [f.lower() for f in booster.feature_names]\n",
    "\n",
    "# XGBoost is sensitive to column order, rearrange their order to be correct\n",
    "df.columns = booster.feature_names\n",
    "\n",
    "# Convert test data to DMatrix for inference\n",
    "dtest = xgb.DMatrix(df)\n",
    "\n",
    "# Run inference\n",
    "y_pred_raw = booster.predict(dtest)\n",
    "y_pred = np.argmax(y_pred_raw, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "906750d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map numeric predictions to class labels\n",
    "label_map = {\n",
    "    0: \"cultural representative\",\n",
    "    1: \"cultural exclusive\",\n",
    "    2: \"cultural agnostic\"\n",
    "}\n",
    "\n",
    "df['label'] = [label_map[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6504a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ~/content/test_out_noLLM.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to a new CSV\n",
    "output_path = '~/content/test_out_noLLM.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e458b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if TEST_ON_VALID_SET:\n",
    "    LABELS = [\"cultural agnostic\", \"cultural representative\", \"cultural exclusive\"]\n",
    "    label_mapping = {\n",
    "        \"cultural agnostic\": 0,\n",
    "        \"cultural representative\": 1,\n",
    "        \"cultural exclusive\": 2\n",
    "    }\n",
    "\n",
    "    # --- 1) Map y_val once (strings → ints) ---\n",
    "    y_val_mapped = pd.Series(y_val).map(label_mapping).values\n",
    "\n",
    "    # --- 2) Take y_pred directly from your argmax output (already ints) ---\n",
    "    #     If it's a list, convert to array:\n",
    "    y_pred_mapped = np.array(y_pred)\n",
    "\n",
    "    # Sanity check\n",
    "    assert y_val_mapped.shape == y_pred_mapped.shape, \\\n",
    "        f\"Shapes mismatch: {y_val_mapped.shape} vs {y_pred_mapped.shape}\"\n",
    "\n",
    "    def evaluate(preds, labels):\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    # === Plotting ===\n",
    "    os.makedirs(\"nonlm_based\", exist_ok=True)\n",
    "    cm = confusion_matrix(\n",
    "        y_val_mapped,\n",
    "        y_pred_mapped,\n",
    "        labels=[0, 1, 2],\n",
    "        normalize=\"true\"\n",
    "    )\n",
    "    df_cm = pd.DataFrame(cm, index=LABELS, columns=LABELS)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=True)\n",
    "    plt.title(\"Confusion Matrix (Ultra Non-LM Based + Stacking)\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"nonlm_based/confusion_matrix_ultra_stacking.png\")\n",
    "    #plt.close()\n",
    "\n",
    "    # === Final Metrics ===\n",
    "    metrics = evaluate(y_pred_mapped, y_val_mapped)\n",
    "    print(\"\\nUltra Stacking Mode Completed!\")\n",
    "    print(f\"Final evaluation metrics: {metrics}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
