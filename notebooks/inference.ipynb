{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6656e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaricando il file 1...\n",
      "--2025-05-03 19:41:12--  https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/dataset_parser.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9788 (9.6K) [text/plain]\n",
      "Saving to: ‘dataset_parser.py.4’\n",
      "\n",
      "dataset_parser.py.4 100%[===================>]   9.56K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-03 19:41:12 (22.4 MB/s) - ‘dataset_parser.py.4’ saved [9788/9788]\n",
      "\n",
      "\n",
      "Scaricando il file 2...\n",
      "--2025-05-03 19:41:12--  https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/wiki_extractor.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6784 (6.6K) [text/plain]\n",
      "Saving to: ‘wiki_extractor.py.4’\n",
      "\n",
      "wiki_extractor.py.4 100%[===================>]   6.62K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-03 19:41:13 (65.9 MB/s) - ‘wiki_extractor.py.4’ saved [6784/6784]\n",
      "\n",
      "\n",
      "Scaricando il file 3...\n",
      "--2025-05-03 19:41:13--  https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/custom_dataset.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8062 (7.9K) [text/plain]\n",
      "Saving to: ‘custom_dataset.py.4’\n",
      "\n",
      "custom_dataset.py.4 100%[===================>]   7.87K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2025-05-03 19:41:13 (2.52 MB/s) - ‘custom_dataset.py.4’ saved [8062/8062]\n",
      "\n",
      "\n",
      "Download completati. Contenuto della directory corrente:\n"
     ]
    }
   ],
   "source": [
    "URL_FILE_1 = \"https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/dataset_parser.py\"\n",
    "URL_FILE_2 = \"https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/wiki_extractor.py\"\n",
    "URL_FILE_3 = \"https://raw.githubusercontent.com/giankev/wikidata_cultural_classifier/refs/heads/main/custom_dataset.py\"\n",
    "\n",
    "print(\"Scaricando il file 1...\")\n",
    "!wget {URL_FILE_1}\n",
    "\n",
    "print(\"\\nScaricando il file 2...\")\n",
    "!wget {URL_FILE_2}\n",
    "\n",
    "print(\"\\nScaricando il file 3...\")\n",
    "!wget {URL_FILE_3}\n",
    "\n",
    "print(\"\\nDownload completati. Contenuto della directory corrente:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909a4c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/leo/cultural_classifier/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: pandas in /home/leo/cultural_classifier/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/leo/cultural_classifier/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: wikidata in /home/leo/cultural_classifier/lib/python3.11/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /home/leo/cultural_classifier/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in /home/leo/cultural_classifier/lib/python3.11/site-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/leo/cultural_classifier/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost pandas scikit-learn wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9654f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from custom_dataset import CustomData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff03887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data non labeled...\n",
      "Adding feature...\n",
      "\n",
      "Fetch Summary:\n",
      "  Attempted: 300\n",
      "  Successful Fetches (Extractor created): 300\n",
      "  Failed Fetches (Extractor is None): 0\n",
      "Feature added...\n",
      "\n",
      "--- Preprocessing DataFrame (Initial rows: 300) ---\n",
      "Handling Missing Values: Nessuna riga con NaN trovata.\n",
      "--- Preprocessing Completo (Final rows: 300, Final columns: 33) ---\n",
      "Test data processed and saved in ~/content/valid_processed.csv.\n",
      "\n",
      "--- Test processed ---\n",
      "Shape of preprocessed data: (300, 33)\n"
     ]
    }
   ],
   "source": [
    "# Percorsi file\n",
    "RAW_TEST_CSV        = '~/content/valid.csv'\n",
    "PROCESSED_TEST_CSV  = '~/content/valid_processed.csv'\n",
    "\n",
    "# Flag per forzare il ricalcolo\n",
    "FORCE_REPROCESS = False\n",
    "\n",
    "df_processed = None\n",
    "\n",
    "# --- Try to load the already processed file, if it exists ---\n",
    "if not FORCE_REPROCESS and os.path.exists(PROCESSED_TEST_CSV):\n",
    "    print(f\"Loading preprocessed data: {PROCESSED_TEST_CSV}...\")\n",
    "    try:\n",
    "        df_processed = pd.read_csv(PROCESSED_TEST_CSV)\n",
    "        print(\"Loading data done.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Loading error {PROCESSED_TEST_CSV}, starting recomputing: {e}\")\n",
    "        df_processed = None  # Force recomputing\n",
    "\n",
    "# --- If not existing or forced to, recompute the features ---\n",
    "if df_processed is None:\n",
    "    print(\"Processing test data non labeled...\")\n",
    "    try:\n",
    "        # 1. Load the initial CSV file\n",
    "        df_test_raw = pd.read_csv(RAW_TEST_CSV)\n",
    "        \n",
    "        # 2. Instantiate preprocessor and add features\n",
    "        processor_test = CustomData(df_test_raw)\n",
    "        df_test_featured = processor_test.add_feature()\n",
    "        \n",
    "        if df_test_featured is not None:\n",
    "            # 3. Apply preprocess (encoding, scaling, ecc.)\n",
    "            df_processed = processor_test.preprocess_data(df_test_featured)\n",
    "            \n",
    "            if df_processed is not None:\n",
    "                # 4. Save result on an output CSV\n",
    "                df_processed.to_csv(PROCESSED_TEST_CSV, index=False)\n",
    "                print(f\"Test data processed and saved in {PROCESSED_TEST_CSV}.\")\n",
    "            else:\n",
    "                print(\"WARNING: Test data is None or empty, saving failed.\")\n",
    "        else:\n",
    "            print(\"ERROR: Failed to add preprocessed features.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during data preprocessing: {e}\")\n",
    "        df_processed = None\n",
    "\n",
    "# --- final check ---\n",
    "if df_processed is not None:\n",
    "    print(\"\\n--- Test processed ---\")\n",
    "    print(f\"Shape of preprocessed data: {df_processed.shape}\")\n",
    "else:\n",
    "    print(\"\\nERROR: could not load preprocessed data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67a719a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data shape: (300, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>page_length</th>\n",
       "      <th>num_links</th>\n",
       "      <th>mean_sitelinks_count</th>\n",
       "      <th>median_sitelinks_count</th>\n",
       "      <th>std_sitelinks_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15786</td>\n",
       "      <td>1. FC Nürnberg</td>\n",
       "      <td>entity</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports club</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>1. FC Nürnberg</td>\n",
       "      <td>14951</td>\n",
       "      <td>92</td>\n",
       "      <td>48.600000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>53.699644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q268530</td>\n",
       "      <td>77 Records</td>\n",
       "      <td>entity</td>\n",
       "      <td>music</td>\n",
       "      <td>record label</td>\n",
       "      <td>cultural exclusive</td>\n",
       "      <td>77 Records</td>\n",
       "      <td>1254</td>\n",
       "      <td>33</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.684420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q216153</td>\n",
       "      <td>A Bug's Life</td>\n",
       "      <td>entity</td>\n",
       "      <td>comics and anime</td>\n",
       "      <td>animated film</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>A Bug's Life</td>\n",
       "      <td>32226</td>\n",
       "      <td>159</td>\n",
       "      <td>46.348387</td>\n",
       "      <td>38.0</td>\n",
       "      <td>40.983017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q593</td>\n",
       "      <td>A Gang Story</td>\n",
       "      <td>entity</td>\n",
       "      <td>films</td>\n",
       "      <td>film</td>\n",
       "      <td>cultural exclusive</td>\n",
       "      <td>A Gang Story</td>\n",
       "      <td>324</td>\n",
       "      <td>6</td>\n",
       "      <td>96.800000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>111.348821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q192185</td>\n",
       "      <td>Aaron Copland</td>\n",
       "      <td>entity</td>\n",
       "      <td>performing arts</td>\n",
       "      <td>choreographer</td>\n",
       "      <td>cultural representative</td>\n",
       "      <td>Aaron Copland</td>\n",
       "      <td>53274</td>\n",
       "      <td>227</td>\n",
       "      <td>34.306667</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.933805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     item            name    type  \\\n",
       "0   http://www.wikidata.org/entity/Q15786  1. FC Nürnberg  entity   \n",
       "1  http://www.wikidata.org/entity/Q268530      77 Records  entity   \n",
       "2  http://www.wikidata.org/entity/Q216153    A Bug's Life  entity   \n",
       "3     http://www.wikidata.org/entity/Q593    A Gang Story  entity   \n",
       "4  http://www.wikidata.org/entity/Q192185   Aaron Copland  entity   \n",
       "\n",
       "           category    subcategory                    label           title  \\\n",
       "0            sports    sports club  cultural representative  1. FC Nürnberg   \n",
       "1             music   record label       cultural exclusive      77 Records   \n",
       "2  comics and anime  animated film  cultural representative    A Bug's Life   \n",
       "3             films           film       cultural exclusive    A Gang Story   \n",
       "4   performing arts  choreographer  cultural representative   Aaron Copland   \n",
       "\n",
       "   page_length  num_links  mean_sitelinks_count  median_sitelinks_count  \\\n",
       "0        14951         92             48.600000                    33.0   \n",
       "1         1254         33             26.875000                    12.0   \n",
       "2        32226        159             46.348387                    38.0   \n",
       "3          324          6             96.800000                    21.0   \n",
       "4        53274        227             34.306667                    25.0   \n",
       "\n",
       "   std_sitelinks_count  \n",
       "0            53.699644  \n",
       "1            38.684420  \n",
       "2            40.983017  \n",
       "3           111.348821  \n",
       "4            36.933805  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test data\n",
    "try:\n",
    "    csv_path = '~/content/valid_augmented.csv'\n",
    "    df_aug = pd.read_csv(csv_path)\n",
    "    print(f\"Augmented data shape: {df_aug.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR - could not load augmented data: {e}\")\n",
    "\n",
    "# Display the first few rows\n",
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4acfeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type',\n",
       " 'label',\n",
       " 'number_sitelinks',\n",
       " 'sitelinks_translation_entropy',\n",
       " 'number_claims',\n",
       " 'po_P495',\n",
       " 'po_P1343',\n",
       " 'po_P2596',\n",
       " 'po_P17',\n",
       " 'number_of_P31',\n",
       " 'sum_cultural_claims',\n",
       " 'po_P172',\n",
       " 'po_P1268',\n",
       " 'po_P136',\n",
       " 'category_architecture',\n",
       " 'category_biology',\n",
       " 'category_books',\n",
       " 'category_comics and anime',\n",
       " 'category_fashion',\n",
       " 'category_films',\n",
       " 'category_food',\n",
       " 'category_geography',\n",
       " 'category_gestures and habits',\n",
       " 'category_history',\n",
       " 'category_literature',\n",
       " 'category_media',\n",
       " 'category_music',\n",
       " 'category_performing arts',\n",
       " 'category_philosophy and religion',\n",
       " 'category_politics',\n",
       " 'category_sports',\n",
       " 'category_transportation',\n",
       " 'category_visual arts']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented data with Custom Data\n",
    "df_processed.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5950778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item',\n",
       " 'name',\n",
       " 'type',\n",
       " 'category',\n",
       " 'subcategory',\n",
       " 'label',\n",
       " 'title',\n",
       " 'page_length',\n",
       " 'num_links',\n",
       " 'mean_sitelinks_count',\n",
       " 'median_sitelinks_count',\n",
       " 'std_sitelinks_count']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded augmented data from wikipedia scraping\n",
    "df_aug.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cddb3961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape train_aug_aligned dopo reindex: (300, 12)\n",
      "Features merged successfully\n",
      "type\n",
      "number_sitelinks\n",
      "sitelinks_translation_entropy\n",
      "number_claims\n",
      "po_p495\n",
      "po_p1343\n",
      "po_p2596\n",
      "po_p17\n",
      "number_of_p31\n",
      "sum_cultural_claims\n",
      "po_p172\n",
      "po_p1268\n",
      "po_p136\n",
      "category_architecture\n",
      "category_biology\n",
      "category_books\n",
      "category_comics and anime\n",
      "category_fashion\n",
      "category_films\n",
      "category_food\n",
      "category_geography\n",
      "category_gestures and habits\n",
      "category_history\n",
      "category_literature\n",
      "category_media\n",
      "category_music\n",
      "category_performing arts\n",
      "category_philosophy and religion\n",
      "category_politics\n",
      "category_sports\n",
      "category_transportation\n",
      "category_visual arts\n",
      "page_length\n",
      "num_links\n",
      "mean_sitelinks_count\n",
      "median_sitelinks_count\n",
      "std_sitelinks_count\n"
     ]
    }
   ],
   "source": [
    "data_aug_aligned = df_aug.reindex(df_processed.index)\n",
    "print(f\"\\nShape train_aug_aligned dopo reindex: {data_aug_aligned.shape}\")\n",
    "\n",
    "if df_processed.index.equals(df_aug.index):\n",
    "    print(\"Features merged successfully\")\n",
    "else:\n",
    "    print(\"ERROR: could not merge features by index\")\n",
    "\n",
    "data_aug_aligned = data_aug_aligned.drop(columns=['item', 'name', 'type', 'category', 'subcategory', 'title'])\n",
    "df_val_aug_concat = pd.concat([df_processed, data_aug_aligned], axis=1)\n",
    "y_val = df_val_aug_concat['label']\n",
    "df_val_aug_concat = df_val_aug_concat.drop(columns=['label'])\n",
    "\n",
    "df_val_aug_concat.columns = df_val_aug_concat.columns.str.lower()\n",
    "\n",
    "features = df_val_aug_concat.columns.tolist()\n",
    "\n",
    "for name in features:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be2672ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before scaling:\n",
      "  Data: (300, 37)\n"
     ]
    }
   ],
   "source": [
    "df = df_val_aug_concat.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"Shape before scaling:\")\n",
    "print(f\"  Data: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7fd3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XGBoost model\n",
    "model_path = '~/content/xgb_best_model_77.json'\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)\n",
    "booster.feature_names = [f.lower() for f in booster.feature_names]\n",
    "\n",
    "df.columns = booster.feature_names\n",
    "\n",
    "# Convert test data to DMatrix for inference\n",
    "dtest = xgb.DMatrix(df)\n",
    "\n",
    "# Run inference\n",
    "y_pred_raw = booster.predict(dtest)\n",
    "y_pred = np.argmax(y_pred_raw, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "906750d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map numeric predictions to class labels\n",
    "label_map = {\n",
    "    0: \"cultural representative\",\n",
    "    1: \"cultural exclusive\",\n",
    "    2: \"cultural agnostic\"\n",
    "}\n",
    "\n",
    "df['label'] = [label_map[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6504a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ~/content/test_out_noLLM.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to a new CSV\n",
    "output_path = '~/content/test_out_noLLM.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e458b0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the first argument must be callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_75832/76651474.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[33m'cultural representative'\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m'cultural exclusive'\u001b[39m: \u001b[32m2\u001b[39m\n\u001b[32m     10\u001b[39m }\n\u001b[32m     11\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m y_val= y_val.map(label_mapping)\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m from sklearn.metrics import (\n\u001b[32m     15\u001b[39m     accuracy_score,\n",
      "\u001b[32m~/cultural_classifier/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, na_action, **kwargs)\u001b[39m\n\u001b[32m  10459\u001b[39m \n\u001b[32m  10460\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.empty:\n\u001b[32m  10461\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.copy()\n\u001b[32m  10462\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10463\u001b[39m         func = functools.partial(func, **kwargs)\n\u001b[32m  10464\u001b[39m \n\u001b[32m  10465\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m infer(x):\n\u001b[32m  10466\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m x._map_values(func, na_action=na_action)\n",
      "\u001b[31mTypeError\u001b[39m: the first argument must be callable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np  # Import numpy\n",
    "\n",
    "label_mapping = {\n",
    "    'cultural agnostic': 0,\n",
    "    'cultural representative': 1,\n",
    "    'cultural exclusive': 2\n",
    "}\n",
    "\n",
    "y_pred = y_pred.map(label_mapping)\n",
    "y_val= y_val.map(label_mapping)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LABELS = [\"cultural agnostic\", \"cultural representative\", \"cultural exclusive\"]\n",
    "\n",
    "def evaluate(preds, labels):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    acc = np.mean(preds == labels)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# === Plotting ===\n",
    "os.makedirs(\"nonlm_based\", exist_ok=True)\n",
    "cm = confusion_matrix(y_val, y_pred, labels=[0, 1, 2], normalize=\"true\")\n",
    "df_cm = pd.DataFrame(cm, index=LABELS, columns=LABELS)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_cm, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=True)\n",
    "plt.title(\"Confusion Matrix (Ultra Non-LM Based + Stacking)\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"nonlm_based/confusion_matrix_ultra_stacking.png\")\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_val, y_pred, target_names=LABELS, zero_division=0)\n",
    "print(report)\n",
    "with open(\"nonlm_based/classification_report_ultra_stacking.json\", \"w\") as f:\n",
    "    json.dump(classification_report(y_val, y_pred, target_names=LABELS, output_dict=True), f, indent=2)\n",
    "\n",
    "print(\"\\n\\u2705 Ultra Stacking Mode Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
